{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper de propiedades de Argenprop\n",
    "\n",
    "Este notebook se utiliza para scrapear las propiedades en alquiler disponibles dentro de la página web Argenprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método get_page. Forma la URL y hace un GET a Argenprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(endpoint, params = None):\n",
    "    BASE_URL = \"https://www.argenprop.com/\"\n",
    "    parameters = []\n",
    "\n",
    "    if params is not None:\n",
    "        page = params.get('page')\n",
    "        if page:\n",
    "            parameters.append(f\"pagina-{page}\")\n",
    "    \n",
    "        sort = params.get('sort')\n",
    "        if sort:\n",
    "            parameters.append(f\"orden-{sort}\")\n",
    "\n",
    "    url = f'{BASE_URL}{endpoint}?{\"&\".join(parameters)}'\n",
    "    \n",
    "    print(f\"[{get_page.__name__}] Obteniendo página: {url}\")\n",
    "    return requests.get(url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método get_property_details obtiene los detalles de la propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_details(url):\n",
    "    response = get_page(url)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    to_return = {\n",
    "        'address': None,\n",
    "        'floor': None,\n",
    "        'neighborhood': None,\n",
    "        'city': None,\n",
    "        'expenses': None,\n",
    "        'price': None,\n",
    "        'sup_cubierta': None,\n",
    "        'sup_total': None\n",
    "    }\n",
    "\n",
    "    # Extracting basic location information safely\n",
    "    titlebar_address = soup.find('h2', class_='titlebar__address')\n",
    "    titlebar_title = soup.find('h2', class_='titlebar__title')\n",
    "\n",
    "    if titlebar_address:\n",
    "        address_parts = titlebar_address.get_text(strip=True).split(',')\n",
    "        to_return['address'] = address_parts[0].strip()\n",
    "        to_return['floor'] = address_parts[1].strip() if len(address_parts) > 1 else None\n",
    "\n",
    "    if titlebar_title:\n",
    "        title_parts = titlebar_title.get_text(strip=True).replace('Alquiler en', '').split(',')\n",
    "        to_return['neighborhood'] = title_parts[0].strip() if len(title_parts) > 0 else None\n",
    "        to_return['city'] = title_parts[1].strip() if len(title_parts) > 1 else None\n",
    "\n",
    "    # Extracting price and expenses\n",
    "    price_tag = soup.find(class_='titlebar__price')\n",
    "    expenses_tag = soup.find(class_='titlebar__expenses')\n",
    "\n",
    "    if price_tag:\n",
    "        price_text = price_tag.get_text(strip=True).replace('$', '').replace('.', '').strip()\n",
    "        try:\n",
    "            to_return['price'] = int(price_text)\n",
    "        except ValueError:\n",
    "            to_return['price'] = None\n",
    "\n",
    "    if expenses_tag:\n",
    "        expenses_text = expenses_tag.get_text(strip=True).replace('+', '').replace('$', '').replace('expensas', '').replace('.', '').strip()\n",
    "        try:\n",
    "            to_return['expenses'] = int(expenses_text)\n",
    "        except ValueError:\n",
    "            to_return['expenses'] = None\n",
    "\n",
    "    # Extracting detailed property features\n",
    "    feature_sections = soup.find_all('ul', class_='property-features')\n",
    "\n",
    "    for ul in feature_sections:\n",
    "        for li in ul.find_all('li'):\n",
    "            p_tag = li.find('p')\n",
    "            strong_tag = li.find('strong')\n",
    "\n",
    "            if not p_tag or not strong_tag:\n",
    "                continue\n",
    "\n",
    "            key = p_tag.get_text(strip=True).split(':')[0].strip().lower().replace(' ', '_').replace('.', '')\n",
    "            value = strong_tag.get_text(strip=True).replace(' m2', '').replace(',', '.').replace('$', '').replace('USD', '').strip()\n",
    "\n",
    "            try:\n",
    "                value = int(value)\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    value = value\n",
    "\n",
    "            to_return[key] = value\n",
    "\n",
    "    return to_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process_property procesa cada propiedad del listado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_property(item):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    url = item.find('a')['href'][1:] if item.find('a') else None # Tomamos a partir del 2do caracter para evitar el /.\n",
    "    advertiser_id = item.find('span', {'data-anunciante-id': True}).get('data-anunciante-id') if item.find('span', {'data-anunciante-id': True}) else None\n",
    "    currency = item.find('span', {'data-moneda': True}).get('data-moneda') if item.find('span', {'data-moneda': True}) else None\n",
    "    price = item.find('span', {'data-precio': True}).get('data-precio') if item.find('span', {'data-precio': True}) else None\n",
    "        \n",
    "    details = get_property_details(url)\n",
    "\n",
    "    # Guardamos la propiedad en el objeto properties.\n",
    "    return {\n",
    "        'property_url': url,\n",
    "        'advertiser_id': advertiser_id,\n",
    "        'currency': currency,\n",
    "        'price': price,\n",
    "        'date_read': timestamp,\n",
    "        **details\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos los filtros que se pueden mandar en la URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Eliminar filtro por rooms\n",
    "\n",
    "listing_type = \"casas-o-departamentos-o-ph\"\n",
    "rent_type = \"alquiler\"\n",
    "rooms = \"1-dormitorio-o-2-dormitorios-o-3-dormitorios-o-4-dormitorios\"\n",
    "\n",
    "#neighborhoods = \"capital-federal\"\n",
    "\n",
    "neighborhoods = \"agronomia-o-br-santa-rita-o-caballito-o-chacarita-o-colegiales-o-flores-o-parque-centenario-o-parque-chacabuco-o-paternal-o-velez-sarsfield-o-villa-crespo-o-villa-del-parque-o-villa-devoto-o-villa-general-mitre-o-villa-urquiza\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el endpoint y los query params iniciales. Nos traemos la primera página del resultado e iteramos por cada página con propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = f'{listing_type}/{rent_type}/{neighborhoods}/{rooms}'\n",
    "params = {\n",
    "    \"page\": 1,\n",
    "    \"sort\": \"menorprecio\"\n",
    "}\n",
    "\n",
    "page = get_page(endpoint, params)\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "listing_items = soup.find_all(class_='listing__item')\n",
    "\n",
    "properties = []\n",
    "\n",
    "# Iteramos hasta que el botón de siguiente página este inactivo.\n",
    "while not \"pagination__page--disable\" in soup.find(class_=['pagination__page-next']).get('class', []):\n",
    "    \n",
    "    # Cada propiedad hace un GET al /show de la misma. \n",
    "    # Agregamos hilos para mejorar el rendimiento.\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        results = executor.map(process_property, listing_items)\n",
    "    \n",
    "    properties.extend(results)\n",
    "\n",
    "    params[\"page\"] += 1\n",
    "    page = get_page(endpoint, params)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    print(f'Procesando página: {params[\"page\"]}')\n",
    "    \n",
    "    listing_items = soup.find_all(class_='listing__item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las propiedades a un df.\n",
    "df = pd.DataFrame(properties)\n",
    "\n",
    "# Guardamos el dataframe en un csv.\n",
    "csv_path = f\"./{datetime.now().strftime('%Y%m%d_%H%M%S')}_propiedades.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "    \n",
    "print(f\"Se guardó el dataframe en: {csv_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
